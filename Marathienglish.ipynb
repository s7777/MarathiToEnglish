{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohyK5M3LiOHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec98c52-c144-489f-8134-899a300810f1"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t## tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'mar.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-marathi.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-marathi.pkl\n",
            "[Go] => [जा]\n",
            "[Run] => [पळ]\n",
            "[Run] => [धाव]\n",
            "[Run] => [पळा]\n",
            "[Run] => [धावा]\n",
            "[Who] => [कोण]\n",
            "[Wow] => [वाह]\n",
            "[Duck] => [खाली वाका]\n",
            "[Fire] => [आग]\n",
            "[Fire] => [फायर]\n",
            "[Help] => [वाचवा]\n",
            "[Help] => [वाचव]\n",
            "[Jump] => [उडी मार]\n",
            "[Jump] => [उडी मारा]\n",
            "[Jump] => [उडी मार]\n",
            "[Jump] => [उडी मारा]\n",
            "[Stop] => [थांबा]\n",
            "[Stop] => [थांब]\n",
            "[Wait] => [थांबा]\n",
            "[Wait] => [थांब]\n",
            "[Hello] => [हॅलो]\n",
            "[Hurry] => [लवकर]\n",
            "[Hurry] => [लवकर कर]\n",
            "[Hurry] => [लवकर करा]\n",
            "[I won] => [मी जिंकलो]\n",
            "[I won] => [मी जिंकले]\n",
            "[Get up] => [ऊठ]\n",
            "[Got it] => [पकडलं]\n",
            "[Got it] => [कळलं]\n",
            "[Got it] => [समजलं]\n",
            "[Got it] => [कळलं का]\n",
            "[Got it] => [समजलं का]\n",
            "[He ran] => [तो पळाला]\n",
            "[He ran] => [ते पळाले]\n",
            "[He ran] => [तो धावला]\n",
            "[He ran] => [ते धावले]\n",
            "[I fell] => [मी पडलो]\n",
            "[I fell] => [मी पडले]\n",
            "[I fell] => [पडलो]\n",
            "[I fell] => [पडले]\n",
            "[I know] => [मला माहीत आहे]\n",
            "[I know] => [माहितीये]\n",
            "[I know] => [माहीत आहे]\n",
            "[I lost] => [मी हरलो]\n",
            "[I lost] => [मी हरले]\n",
            "[I spit] => [मी थुकतो]\n",
            "[I spit] => [मी थुकते]\n",
            "[I work] => [मी काम करतो]\n",
            "[I work] => [मी काम करते]\n",
            "[Im OK] => [मी ठीक आहे]\n",
            "[Listen] => [ऐक]\n",
            "[Listen] => [ऐका]\n",
            "[No way] => [शक्यच नाही]\n",
            "[Really] => [खरोखर]\n",
            "[Really] => [खरंच]\n",
            "[Really] => [खरंच का]\n",
            "[Thanks] => [धन्यवाद]\n",
            "[We won] => [आम्ही जिंकलो]\n",
            "[We won] => [आपण जिंकलो]\n",
            "[Why me] => [मीच का]\n",
            "[Why me] => [मी का]\n",
            "[Ask Tom] => [टॉमला विचार]\n",
            "[Ask Tom] => [टॉमला विचारा]\n",
            "[Call me] => [मला बोलव]\n",
            "[Call me] => [मला बोलवा]\n",
            "[Call me] => [मला फोन करा]\n",
            "[Call me] => [मला फोन कर]\n",
            "[Call us] => [आम्हाला फोन कर]\n",
            "[Call us] => [आम्हाला फोन करा]\n",
            "[Come in] => [आत ये]\n",
            "[Come on] => [चल]\n",
            "[Come on] => [चला]\n",
            "[Fold it] => [घडी घाल]\n",
            "[Fold it] => [घडी घाला]\n",
            "[Get Tom] => [टॉमला घे]\n",
            "[Get Tom] => [टॉमला आण]\n",
            "[Get Tom] => [टॉमला पकड]\n",
            "[Get out] => [बाहेर हो]\n",
            "[Get out] => [बाहेर व्हा]\n",
            "[Go home] => [घरी जा]\n",
            "[He came] => [तो आला]\n",
            "[He came] => [ते आले]\n",
            "[He left] => [तो निघाला]\n",
            "[He left] => [ते निघाले]\n",
            "[He runs] => [तो पळतो]\n",
            "[He runs] => [ते पळतात]\n",
            "[He runs] => [तो धावतो]\n",
            "[He runs] => [ते धावतात]\n",
            "[Help me] => [वाचवा]\n",
            "[Help me] => [वाचव]\n",
            "[Help me] => [माझी मदत करा]\n",
            "[Help me] => [मला वाचव]\n",
            "[Help me] => [मला वाचवा]\n",
            "[Help us] => [आमची मदत करा]\n",
            "[Help us] => [आमची मदत कर]\n",
            "[Help us] => [आम्हाला वाचव]\n",
            "[I waved] => [मी हात हलवला]\n",
            "[Im Tom] => [मी टॉम]\n",
            "[Im Tom] => [टॉम मीच]\n",
            "[Im Tom] => [मी टॉम आहे]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t__mR0AqfCX0",
        "outputId": "438944b6-2cdf-4194-c175-ef3f194855d9"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-marathi.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 20000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:19000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-marathi-both.pkl')\n",
        "save_clean_data(train, 'english-marathi-train.pkl')\n",
        "save_clean_data(test, 'english-marathi-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-marathi-both.pkl\n",
            "Saved: english-marathi-train.pkl\n",
            "Saved: english-marathi-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "B2ep7Df7fY0I",
        "outputId": "ada479b4-16ac-45ef-ce27-6b77c9dd663e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-marathi-both.pkl')\n",
        "train = load_clean_sentences('english-marathi-train.pkl')\n",
        "test = load_clean_sentences('english-marathi-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Marathi Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Marathi Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-001d57ec4f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvz6cGxCN6gd",
        "outputId": "6fa993b2-d59b-4a68-ff86-d2639d7fbadc"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-marathi-both.pkl')\n",
        "train = load_clean_sentences('english-marathi-train.pkl')\n",
        "test = load_clean_sentences('english-marathi-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Marathi Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Marathi Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3015\n",
            "English Max Length: 7\n",
            "Marathi Vocabulary Size: 6319\n",
            "Marathi Max Length: 9\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 9, 512)            3235328   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 7, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 7, 512)            2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 7, 3015)           1546695   \n",
            "=================================================================\n",
            "Total params: 8,980,423\n",
            "Trainable params: 8,980,423\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "297/297 - 207s - loss: 3.5520 - val_loss: 3.1096\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.10960, saving model to model.h5\n",
            "Epoch 2/100\n",
            "297/297 - 200s - loss: 2.9261 - val_loss: 2.6287\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.10960 to 2.62874, saving model to model.h5\n",
            "Epoch 3/100\n",
            "297/297 - 200s - loss: 2.4438 - val_loss: 2.1515\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.62874 to 2.15148, saving model to model.h5\n",
            "Epoch 4/100\n",
            "297/297 - 202s - loss: 2.0221 - val_loss: 1.7763\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.15148 to 1.77628, saving model to model.h5\n",
            "Epoch 5/100\n",
            "297/297 - 201s - loss: 1.6713 - val_loss: 1.4325\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.77628 to 1.43247, saving model to model.h5\n",
            "Epoch 6/100\n",
            "297/297 - 200s - loss: 1.3721 - val_loss: 1.1791\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.43247 to 1.17908, saving model to model.h5\n",
            "Epoch 7/100\n",
            "297/297 - 201s - loss: 1.1212 - val_loss: 0.9491\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.17908 to 0.94910, saving model to model.h5\n",
            "Epoch 8/100\n",
            "297/297 - 200s - loss: 0.9084 - val_loss: 0.7522\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.94910 to 0.75217, saving model to model.h5\n",
            "Epoch 9/100\n",
            "297/297 - 200s - loss: 0.7218 - val_loss: 0.6045\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75217 to 0.60449, saving model to model.h5\n",
            "Epoch 10/100\n",
            "297/297 - 202s - loss: 0.5700 - val_loss: 0.4745\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.60449 to 0.47454, saving model to model.h5\n",
            "Epoch 11/100\n",
            "297/297 - 200s - loss: 0.4463 - val_loss: 0.3844\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.47454 to 0.38441, saving model to model.h5\n",
            "Epoch 12/100\n",
            "297/297 - 200s - loss: 0.3512 - val_loss: 0.3067\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.38441 to 0.30668, saving model to model.h5\n",
            "Epoch 13/100\n",
            "297/297 - 201s - loss: 0.2730 - val_loss: 0.2503\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.30668 to 0.25028, saving model to model.h5\n",
            "Epoch 14/100\n",
            "297/297 - 200s - loss: 0.2121 - val_loss: 0.2125\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.25028 to 0.21246, saving model to model.h5\n",
            "Epoch 15/100\n",
            "297/297 - 200s - loss: 0.1679 - val_loss: 0.1737\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.21246 to 0.17373, saving model to model.h5\n",
            "Epoch 16/100\n",
            "297/297 - 201s - loss: 0.1354 - val_loss: 0.1537\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.17373 to 0.15368, saving model to model.h5\n",
            "Epoch 17/100\n",
            "297/297 - 200s - loss: 0.1123 - val_loss: 0.1358\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.15368 to 0.13581, saving model to model.h5\n",
            "Epoch 18/100\n",
            "297/297 - 199s - loss: 0.0940 - val_loss: 0.1273\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.13581 to 0.12734, saving model to model.h5\n",
            "Epoch 19/100\n",
            "297/297 - 200s - loss: 0.0811 - val_loss: 0.1164\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.12734 to 0.11638, saving model to model.h5\n",
            "Epoch 20/100\n",
            "297/297 - 200s - loss: 0.0705 - val_loss: 0.1117\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.11638 to 0.11168, saving model to model.h5\n",
            "Epoch 21/100\n",
            "297/297 - 199s - loss: 0.0664 - val_loss: 0.1086\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.11168 to 0.10861, saving model to model.h5\n",
            "Epoch 22/100\n",
            "297/297 - 201s - loss: 0.0623 - val_loss: 0.1069\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.10861 to 0.10687, saving model to model.h5\n",
            "Epoch 23/100\n",
            "297/297 - 200s - loss: 0.0627 - val_loss: 0.1033\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.10687 to 0.10333, saving model to model.h5\n",
            "Epoch 24/100\n",
            "297/297 - 200s - loss: 0.0565 - val_loss: 0.1035\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.10333\n",
            "Epoch 25/100\n",
            "297/297 - 201s - loss: 0.0575 - val_loss: 0.1066\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.10333\n",
            "Epoch 26/100\n",
            "297/297 - 200s - loss: 0.0588 - val_loss: 0.1068\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.10333\n",
            "Epoch 27/100\n",
            "297/297 - 200s - loss: 0.0564 - val_loss: 0.1043\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.10333\n",
            "Epoch 28/100\n",
            "297/297 - 201s - loss: 0.0543 - val_loss: 0.0999\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.10333 to 0.09992, saving model to model.h5\n",
            "Epoch 29/100\n",
            "297/297 - 200s - loss: 0.0513 - val_loss: 0.0994\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.09992 to 0.09942, saving model to model.h5\n",
            "Epoch 30/100\n",
            "297/297 - 200s - loss: 0.0493 - val_loss: 0.0978\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.09942 to 0.09783, saving model to model.h5\n",
            "Epoch 31/100\n",
            "297/297 - 201s - loss: 0.0476 - val_loss: 0.0980\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.09783\n",
            "Epoch 32/100\n",
            "297/297 - 201s - loss: 0.0469 - val_loss: 0.0983\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.09783\n",
            "Epoch 33/100\n",
            "297/297 - 200s - loss: 0.0456 - val_loss: 0.0942\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.09783 to 0.09425, saving model to model.h5\n",
            "Epoch 34/100\n",
            "297/297 - 200s - loss: 0.0451 - val_loss: 0.0986\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.09425\n",
            "Epoch 35/100\n",
            "297/297 - 200s - loss: 0.0453 - val_loss: 0.0976\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.09425\n",
            "Epoch 36/100\n",
            "297/297 - 199s - loss: 0.0458 - val_loss: 0.0967\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.09425\n",
            "Epoch 37/100\n",
            "297/297 - 201s - loss: 0.0452 - val_loss: 0.0959\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.09425\n",
            "Epoch 38/100\n",
            "297/297 - 200s - loss: 0.0462 - val_loss: 0.1010\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.09425\n",
            "Epoch 39/100\n",
            "297/297 - 200s - loss: 0.0491 - val_loss: 0.0976\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.09425\n",
            "Epoch 40/100\n",
            "297/297 - 200s - loss: 0.0438 - val_loss: 0.0924\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.09425 to 0.09243, saving model to model.h5\n",
            "Epoch 41/100\n",
            "297/297 - 200s - loss: 0.0405 - val_loss: 0.0921\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.09243 to 0.09213, saving model to model.h5\n",
            "Epoch 42/100\n",
            "297/297 - 200s - loss: 0.0381 - val_loss: 0.0907\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.09213 to 0.09067, saving model to model.h5\n",
            "Epoch 43/100\n",
            "297/297 - 201s - loss: 0.0373 - val_loss: 0.0874\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.09067 to 0.08740, saving model to model.h5\n",
            "Epoch 44/100\n",
            "297/297 - 201s - loss: 0.0356 - val_loss: 0.0894\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.08740\n",
            "Epoch 45/100\n",
            "297/297 - 200s - loss: 0.0364 - val_loss: 0.0903\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.08740\n",
            "Epoch 46/100\n",
            "297/297 - 201s - loss: 0.0387 - val_loss: 0.0975\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.08740\n",
            "Epoch 47/100\n",
            "297/297 - 201s - loss: 0.0461 - val_loss: 0.1041\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.08740\n",
            "Epoch 48/100\n",
            "297/297 - 201s - loss: 0.0503 - val_loss: 0.1065\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.08740\n",
            "Epoch 49/100\n",
            "297/297 - 201s - loss: 0.0496 - val_loss: 0.0992\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.08740\n",
            "Epoch 50/100\n",
            "297/297 - 201s - loss: 0.0406 - val_loss: 0.0939\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.08740\n",
            "Epoch 51/100\n",
            "297/297 - 201s - loss: 0.0365 - val_loss: 0.0906\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.08740\n",
            "Epoch 52/100\n",
            "297/297 - 202s - loss: 0.0331 - val_loss: 0.0892\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.08740\n",
            "Epoch 53/100\n",
            "297/297 - 201s - loss: 0.0321 - val_loss: 0.0876\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.08740\n",
            "Epoch 54/100\n",
            "297/297 - 201s - loss: 0.0313 - val_loss: 0.0869\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.08740 to 0.08690, saving model to model.h5\n",
            "Epoch 55/100\n",
            "297/297 - 201s - loss: 0.0313 - val_loss: 0.0869\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.08690 to 0.08689, saving model to model.h5\n",
            "Epoch 56/100\n",
            "297/297 - 200s - loss: 0.0321 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.08689\n",
            "Epoch 57/100\n",
            "297/297 - 200s - loss: 0.0323 - val_loss: 0.0912\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.08689\n",
            "Epoch 58/100\n",
            "297/297 - 200s - loss: 0.0340 - val_loss: 0.0928\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.08689\n",
            "Epoch 59/100\n",
            "297/297 - 199s - loss: 0.0554 - val_loss: 0.1312\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.08689\n",
            "Epoch 60/100\n",
            "297/297 - 200s - loss: 0.0659 - val_loss: 0.1038\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.08689\n",
            "Epoch 61/100\n",
            "297/297 - 200s - loss: 0.0414 - val_loss: 0.0916\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.08689\n",
            "Epoch 62/100\n",
            "297/297 - 201s - loss: 0.0332 - val_loss: 0.0884\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.08689\n",
            "Epoch 63/100\n",
            "297/297 - 201s - loss: 0.0307 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.08689\n",
            "Epoch 64/100\n",
            "297/297 - 201s - loss: 0.0300 - val_loss: 0.0862\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.08689 to 0.08617, saving model to model.h5\n",
            "Epoch 65/100\n",
            "297/297 - 201s - loss: 0.0293 - val_loss: 0.0871\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.08617\n",
            "Epoch 66/100\n",
            "297/297 - 202s - loss: 0.0292 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.08617\n",
            "Epoch 67/100\n",
            "297/297 - 203s - loss: 0.0290 - val_loss: 0.0872\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.08617\n",
            "Epoch 68/100\n",
            "297/297 - 203s - loss: 0.0289 - val_loss: 0.0875\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.08617\n",
            "Epoch 69/100\n",
            "297/297 - 202s - loss: 0.0295 - val_loss: 0.0881\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.08617\n",
            "Epoch 70/100\n",
            "297/297 - 204s - loss: 0.0308 - val_loss: 0.0891\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.08617\n",
            "Epoch 71/100\n",
            "297/297 - 203s - loss: 0.0318 - val_loss: 0.0920\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.08617\n",
            "Epoch 72/100\n",
            "297/297 - 202s - loss: 0.0423 - val_loss: 0.1171\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.08617\n",
            "Epoch 73/100\n",
            "297/297 - 204s - loss: 0.0602 - val_loss: 0.1075\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.08617\n",
            "Epoch 74/100\n",
            "297/297 - 203s - loss: 0.0454 - val_loss: 0.0925\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.08617\n",
            "Epoch 75/100\n",
            "297/297 - 202s - loss: 0.0343 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.08617\n",
            "Epoch 76/100\n",
            "297/297 - 204s - loss: 0.0307 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.08617\n",
            "Epoch 77/100\n",
            "297/297 - 203s - loss: 0.0298 - val_loss: 0.0872\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.08617\n",
            "Epoch 78/100\n",
            "297/297 - 202s - loss: 0.0291 - val_loss: 0.0871\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.08617\n",
            "Epoch 79/100\n",
            "297/297 - 201s - loss: 0.0278 - val_loss: 0.0869\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.08617\n",
            "Epoch 80/100\n",
            "297/297 - 202s - loss: 0.0279 - val_loss: 0.0863\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.08617\n",
            "Epoch 81/100\n",
            "297/297 - 201s - loss: 0.0279 - val_loss: 0.0876\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.08617\n",
            "Epoch 82/100\n",
            "297/297 - 201s - loss: 0.0294 - val_loss: 0.0895\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.08617\n",
            "Epoch 83/100\n",
            "297/297 - 202s - loss: 0.0318 - val_loss: 0.0901\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.08617\n",
            "Epoch 84/100\n",
            "297/297 - 200s - loss: 0.0343 - val_loss: 0.0972\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.08617\n",
            "Epoch 85/100\n",
            "297/297 - 200s - loss: 0.0427 - val_loss: 0.1039\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.08617\n",
            "Epoch 86/100\n",
            "297/297 - 199s - loss: 0.0421 - val_loss: 0.0978\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.08617\n",
            "Epoch 87/100\n",
            "297/297 - 200s - loss: 0.0348 - val_loss: 0.0923\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.08617\n",
            "Epoch 88/100\n",
            "297/297 - 199s - loss: 0.0317 - val_loss: 0.0907\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.08617\n",
            "Epoch 89/100\n",
            "297/297 - 199s - loss: 0.0296 - val_loss: 0.0883\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.08617\n",
            "Epoch 90/100\n",
            "297/297 - 199s - loss: 0.0283 - val_loss: 0.0885\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.08617\n",
            "Epoch 91/100\n",
            "297/297 - 200s - loss: 0.0275 - val_loss: 0.0887\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.08617\n",
            "Epoch 92/100\n",
            "297/297 - 200s - loss: 0.0276 - val_loss: 0.0880\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.08617\n",
            "Epoch 93/100\n",
            "297/297 - 200s - loss: 0.0278 - val_loss: 0.0880\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.08617\n",
            "Epoch 94/100\n",
            "297/297 - 200s - loss: 0.0275 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.08617\n",
            "Epoch 95/100\n",
            "297/297 - 200s - loss: 0.0277 - val_loss: 0.0878\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.08617\n",
            "Epoch 96/100\n",
            "297/297 - 200s - loss: 0.0277 - val_loss: 0.0887\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.08617\n",
            "Epoch 97/100\n",
            "297/297 - 200s - loss: 0.0286 - val_loss: 0.0898\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.08617\n",
            "Epoch 98/100\n",
            "297/297 - 200s - loss: 0.0294 - val_loss: 0.0915\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.08617\n",
            "Epoch 99/100\n",
            "297/297 - 201s - loss: 0.0463 - val_loss: 0.1262\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.08617\n",
            "Epoch 100/100\n",
            "297/297 - 200s - loss: 0.0585 - val_loss: 0.1017\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.08617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdc5e3cf278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUyLJUhBxFR-",
        "outputId": "23f4ef4f-26b5-40fc-cb09-c9a40d6552d1"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-marathi-both.pkl')\n",
        "train = load_clean_sentences('english-marathi-train.pkl')\n",
        "test = load_clean_sentences('english-marathi-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[तो पळाला], target=[He ran], predicted=[he ran]\n",
            "src=[तो माझा भाऊ आहे], target=[He is my brother], predicted=[hes my brother]\n",
            "src=[हे कोणासाठी आहेत], target=[Who are these for], predicted=[who are these for]\n",
            "src=[ही पेन्सिल तुझी आहे का], target=[Is this pencil yours], predicted=[is this pencil yours]\n",
            "src=[सर्वच सफरचंद लाल असतात], target=[All apples are red], predicted=[all apples are red]\n",
            "src=[टॉम का], target=[Is this Tom], predicted=[is this tom]\n",
            "src=[गजर वाजला], target=[The alarm went off], predicted=[the alarm went off]\n",
            "src=[हे केस आहेत], target=[This is hair], predicted=[this is hair]\n",
            "src=[मला जायला लागलं तर मी जाईन], target=[I will go if I must], predicted=[i will go if i must]\n",
            "src=[ती मी होते], target=[That was me], predicted=[that was me]\n",
            "BLEU-1: 0.698389\n",
            "BLEU-2: 0.638046\n",
            "BLEU-3: 0.585879\n",
            "BLEU-4: 0.412369\n",
            "test\n",
            "src=[पुस्तक माझं आहे], target=[The book is mine], predicted=[the book is mine]\n",
            "src=[तुम्ही आत्ता एकटे आहात का], target=[Are you alone right now], predicted=[are you alone right now]\n",
            "src=[तुझा कुत्रा इकडे आहे], target=[Your dog is here], predicted=[your dog is here]\n",
            "src=[त्याने त्याची टोपी काढली], target=[He took off his hat], predicted=[he took off his hat]\n",
            "src=[मी तुम्हाला हरवू शकते], target=[I can beat you], predicted=[i can beat you]\n",
            "src=[मी जेवण गरम करतेय], target=[I am heating the dinner], predicted=[i am heating the dinner]\n",
            "src=[मला ही थंडी सहन होत नाहीये], target=[I cant stand this cold], predicted=[i cant stand this cold]\n",
            "src=[तिने स्वतःच त्याची मदत केली], target=[She herself helped him], predicted=[she herself helped him]\n",
            "src=[टॉम अगदी वेगाने पळतो], target=[Tom runs very fast], predicted=[tom runs very fast]\n",
            "src=[तो त्याचा मित्र आहे], target=[He is his friend], predicted=[he is his friend]\n",
            "BLEU-1: 0.687956\n",
            "BLEU-2: 0.626645\n",
            "BLEU-3: 0.575475\n",
            "BLEU-4: 0.403827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T-EwQDJQe6U",
        "outputId": "3b3d113c-1ec8-41f2-9495-6faddfe4a231"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-marathi-both.pkl')\n",
        "train = load_clean_sentences('english-marathi-train.pkl')\n",
        "test = load_clean_sentences('english-marathi-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[तो पळाला], target=[He ran], predicted=[he ran]\n",
            "src=[तो माझा भाऊ आहे], target=[He is my brother], predicted=[this is brother]\n",
            "src=[हे कोणासाठी आहेत], target=[Who are these for], predicted=[who are these for]\n",
            "src=[ही पेन्सिल तुझी आहे का], target=[Is this pencil yours], predicted=[is this pencil yours]\n",
            "src=[सर्वच सफरचंद लाल असतात], target=[All apples are red], predicted=[all apples are red]\n",
            "src=[टॉम का], target=[Is this Tom], predicted=[is this tom]\n",
            "src=[गजर वाजला], target=[The alarm went off], predicted=[the alarm went off]\n",
            "src=[हे केस आहेत], target=[This is hair], predicted=[this is hair]\n",
            "src=[मला जायला लागलं तर मी जाईन], target=[I will go if I must], predicted=[i will go if i must]\n",
            "src=[ती मी होते], target=[That was me], predicted=[that was me]\n",
            "BLEU-1: 0.698513\n",
            "BLEU-2: 0.637991\n",
            "BLEU-3: 0.585915\n",
            "BLEU-4: 0.412292\n",
            "test\n",
            "src=[पुस्तक माझं आहे], target=[The book is mine], predicted=[the book is mine]\n",
            "src=[तुम्ही आत्ता एकटे आहात का], target=[Are you alone right now], predicted=[are you alone right now]\n",
            "src=[तुझा कुत्रा इकडे आहे], target=[Your dog is here], predicted=[your dog is here]\n",
            "src=[त्याने त्याची टोपी काढली], target=[He took off his hat], predicted=[he took off his hat]\n",
            "src=[मी तुम्हाला हरवू शकते], target=[I can beat you], predicted=[i can beat you]\n",
            "src=[मी जेवण गरम करतेय], target=[I am heating the dinner], predicted=[i am heating the dinner]\n",
            "src=[मला ही थंडी सहन होत नाहीये], target=[I cant stand this cold], predicted=[i cant stand this cold]\n",
            "src=[तिने स्वतःच त्याची मदत केली], target=[She herself helped him], predicted=[she herself helped him]\n",
            "src=[टॉम अगदी वेगाने पळतो], target=[Tom runs very fast], predicted=[tom runs very fast]\n",
            "src=[तो त्याचा मित्र आहे], target=[He is his friend], predicted=[he is his friend]\n",
            "BLEU-1: 0.689029\n",
            "BLEU-2: 0.627474\n",
            "BLEU-3: 0.576046\n",
            "BLEU-4: 0.404108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHaO6sYEvzN6",
        "outputId": "a0fa2507-9504-4efc-da6f-0ebf5053f974"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t## tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'tam.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-tam.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-tam.pkl\n",
            "[I slept] => [நான் தூங்கினேன்]\n",
            "[Calm down] => [அமைதியாக இருங்கள்]\n",
            "[Ill walk] => [நான் நடப்பேன்]\n",
            "[Who is he] => [அவன் யார்]\n",
            "[Who knows] => [யாருக்குத் தெரியும்]\n",
            "[She smiled] => [அவள் சிரித்தாள்]\n",
            "[Talk to me] => [என்னிடம் பேசு]\n",
            "[Who is she] => [அவள் யார்]\n",
            "[Go to sleep] => [போய் தூங்கு]\n",
            "[It may rain] => [மழை பெய்யலாம்]\n",
            "[She bit him] => [அவள் அவனைக் கடித்தாள்]\n",
            "[She hit him] => [அவள் அவனைக் அடித்தாள்]\n",
            "[She is kind] => [அவள் அன்பானவள்]\n",
            "[She is eight] => [அவளுக்கு எட்டு வயது]\n",
            "[Where are we] => [நாம் எங்கே இருக்கிறோம்]\n",
            "[Keep in touch] => [தொடர்பில் இரு]\n",
            "[See you again] => [மறுபடியும் சந்திப்போம்]\n",
            "[Give it to her] => [அவளிடம் கொடு]\n",
            "[I ate too much] => [நான் நிறைய சாப்பிட்டேன்]\n",
            "[Ill see to it] => [அதை நான் பார்க்கிறேன்]\n",
            "[Its up to you] => [உன் கையில்தான் இருக்கிறது]\n",
            "[Leave it to me] => [என்னிடம் விட்டுவிடு]\n",
            "[Listen to this] => [இதைக் கேள்]\n",
            "[Thats the way] => [அந்த பக்கம்தான் வழி]\n",
            "[Come and see me] => [என்னை வந்து பார்]\n",
            "[Dont lie to me] => [என்னிடம் பொய் சொல்லாதே]\n",
            "[He began to run] => [அவன் ஓட ஆரம்பித்தான்]\n",
            "[He just arrived] => [அவன் இப்பொழுதுதான் வந்தான்]\n",
            "[He likes to run] => [அவன் ஓட விருப்பப் படுகிறான்]\n",
            "[How is your dad] => [தங்களுடைய தந்தையார் எப்படி இருக்கிறார்கள்]\n",
            "[I want to sleep] => [நான் தூங்க விரும்புகிறேன்]\n",
            "[Im able to run] => [என்னால் ஓட முடிகிறது]\n",
            "[Raise your hand] => [கையைத் தூக்கு]\n",
            "[What did he say] => [அவன் என்ன சொன்னான்]\n",
            "[When can we eat] => [எப்பொழுது நம்மால் சாப்பிட முடியும்]\n",
            "[Come and help us] => [வா எங்களுக்கு உதவி செய்]\n",
            "[He is still here] => [அவன் இன்னும் இருக்கிறான்]\n",
            "[I have to go now] => [நான் இப்பொழுது போக வேண்டும்]\n",
            "[I know that much] => [எனக்கு அவ்வளவு தெரியும்]\n",
            "[I made a mistake] => [நான் ஒரு தவறு செய்தேன்]\n",
            "[I walk to school] => [நான் பள்ளிக்கு நடந்து செல்கிறேன்]\n",
            "[Thats our house] => [அது எங்களுடைய வீடு]\n",
            "[Those are my CDs] => [அவைகள் என்னுடைய CD கள்]\n",
            "[Walk ahead of me] => [எனக்கு முன்னால் நட]\n",
            "[Well follow you] => [நாங்கள் உன்னைத் பின்பற்றுவோம் அ தொடர்வோம்]\n",
            "[Beware of the dog] => [நாய் ஜாக்கிரதை]\n",
            "[He came back soon] => [அவன் சீக்கிரம் திரும்பி வந்தான்]\n",
            "[He has three sons] => [அவருக்கு மூன்று மகன்கள்]\n",
            "[I know how to ski] => [எப்படி பனியில் சறுக்கி விளையாடுவது என்பது எனக்கு தெரியும்]\n",
            "[I know what to do] => [என்ன செய்வது என்பது எனக்குத் தெரியும்]\n",
            "[Im kind of happy] => [நான் ஒரு விதமான மகிழ்ச்சியிலிருக்கிறேன்]\n",
            "[Keep to the right] => [வலது பக்கத்தை கடைப் பிடி]\n",
            "[She began to sing] => [அவள் பாட ஆரம்பித்தாள்]\n",
            "[She decided to go] => [அவள் போகத் தீர்மானித்தாள்]\n",
            "[Do I have to study] => [நான் படிக்க வேண்டுமா]\n",
            "[He is sure to come] => [அவன் வருவது நிச்சயம்]\n",
            "[I had to walk home] => [நான் வீட்டிற்கு நடக்க வேண்டியிருந்தது]\n",
            "[I have to dress up] => [நான் ஆடை அணிய வேண்டும்]\n",
            "[I told him to come] => [நான் அவனை வரச் சொன்னேன்]\n",
            "[Im short of money] => [என்னிடம் பணம் குறைவாக இருக்கிறது]\n",
            "[May I speak to you] => [நான் உன்னிடம் பேசலாமா]\n",
            "[She gave it to him] => [அவள் இதை அவனுக்குக் கொடுத்தாள்]\n",
            "[She is kind to him] => [அவள் அவனிடம் அன்பாக இருக்கிறாள்]\n",
            "[She sat next to me] => [அவள் எனக்கு அருகில் அமர்ந்தாள்]\n",
            "[Shut up and listen] => [வாயை மூடி கவனி]\n",
            "[Tell me what to do] => [நான் என்ன செய்ய வேண்டும் என்று சொல்]\n",
            "[Tom runs very fast] => [டாம் ரொம்ப வேகமாக ஓடுகிறான்]\n",
            "[We ran out of food] => [எங்களுக்கு உணவு தட்டுப்பாடு ஏற்பட்டது]\n",
            "[We started to walk] => [நாங்கள் நடக்க ஆரம்பித்தோம்]\n",
            "[When does it begin] => [இது எப்பொழுது ஆரம்பிக்கிறது]\n",
            "[Are you ready to go] => [நீங்கள் போகத் தயாராக இருக்கிறீர்களா]\n",
            "[Do you have any gum] => [உன்னிடம் ஏதாவது பசை இருக்கிறதா]\n",
            "[Does she play piano] => [அவள் பியானோ வாசிக்கிறாளோ]\n",
            "[Dont listen to her] => [அவள் சொல்வதைக் கேட்காதீர்]\n",
            "[Go and wake Mary up] => [போய் மேரியை எழுப்பு]\n",
            "[He seems to know us] => [அவனுக்கு நம்மைப் தெரியும் என்று தோன்றுகிறது]\n",
            "[I am engaged to her] => [எனக்கு அவளோடு நிச்சயமாகியிருக்கு]\n",
            "[I have to leave now] => [நான் இப்பொழுது கிளம்ப வேண்டும்]\n",
            "[I want to go abroad] => [நான் வெளி நாட்டிற்குச் செல்ல விரும்புகிறேன்]\n",
            "[Im glad to see you] => [உன்னைப் பார்ப்பதில் நான் மகிழ்ச்சி அடைகிறேன்]\n",
            "[Im proud of my son] => [என் மகனைப் பற்றி பெருமைப் படுகிறேன்]\n",
            "[Im taller than you] => [நான் உன்னை விட உயரமாக இருக்கிறேன்]\n",
            "[Im trying to sleep] => [நான் தூங்குவதற்கு முயற்சி செய்து கொண்டிருக்கிறேன்]\n",
            "[Its free of charge] => [இதற்கு கட்டணமில்லை]\n",
            "[Its time to get up] => [தூக்கத்திலிருந்து எழுவதற்கான நேரம் இது]\n",
            "[Nobody speaks to me] => [என் கூட யாரும் பேசுவதில்லை]\n",
            "[Roll the ball to me] => [பந்தை என்னிடம் உருட்டி விடு]\n",
            "[She boiled the eggs] => [அவள் முட்டைகளை வேக வைத்தாள்]\n",
            "[She danced with him] => [அவள் அவனோடு நடனம் ஆடினாள்]\n",
            "[She gave him a book] => [அவள் அவனுக்கு ஒரு புத்தகத்தைக் கொடுத்தாள்]\n",
            "[She has 2000 books] => [அவளிடம் 2000 புத்தகங்கள் உள்ளன]\n",
            "[This apple is sweet] => [இந்த ஆப்பிள் இனிப்பாக இருக்கிறது]\n",
            "[We swam in the lake] => [அவன் ஏரியில் நீச்சலடித்தான்]\n",
            "[Come home before six] => [ஆறு மணிக்கு முன்பு வீட் டிற்கு வா]\n",
            "[Go and see who it is] => [போய் யார் என்று பார்]\n",
            "[I am afraid of bears] => [எனக்குக் கரடிகளைக கண்டால் பயம்]\n",
            "[I expect him to come] => [அவன் வருவான் என எதிர் பார்க்கிறேன்]\n",
            "[Its a piece of cake] => [இது ஒரு கேக்கின் துண்டு]\n",
            "[The boy began to cry] => [அந்த பையன் அழ ஆரம்பித்தான்]\n",
            "[You keep out of this] => [நீ இதில் தலையிடாதே]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3HZNb33wMQh",
        "outputId": "c78d62c8-5f88-45aa-cd10-a8d7fbc4d006"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-tam.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-tam-both.pkl')\n",
        "save_clean_data(train, 'english-tam-train.pkl')\n",
        "save_clean_data(test, 'english-tam-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-tam-both.pkl\n",
            "Saved: english-tam-train.pkl\n",
            "Saved: english-tam-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbISuB0QwmBs",
        "outputId": "13ec4aeb-e414-46f5-b7fc-a7634fbf6885"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-tam-both.pkl')\n",
        "train = load_clean_sentences('english-tam-train.pkl')\n",
        "test = load_clean_sentences('english-tam-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Tamil Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Tamil Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 385\n",
            "English Max Length: 19\n",
            "Tamil Vocabulary Size: 552\n",
            "Tamil Max Length: 11\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 512)           282624    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 19, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 19, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 19, 385)           197505    \n",
            "=================================================================\n",
            "Total params: 4,678,529\n",
            "Trainable params: 4,678,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "4/4 - 8s - loss: 5.9052\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 2/100\n",
            "4/4 - 3s - loss: 3.5631\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 3/100\n",
            "4/4 - 3s - loss: 2.2325\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 4/100\n",
            "4/4 - 3s - loss: 1.9922\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 5/100\n",
            "4/4 - 3s - loss: 1.8304\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 6/100\n",
            "4/4 - 3s - loss: 1.8439\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 7/100\n",
            "4/4 - 3s - loss: 1.7833\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 8/100\n",
            "4/4 - 3s - loss: 1.7916\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 9/100\n",
            "4/4 - 3s - loss: 1.8753\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 10/100\n",
            "4/4 - 3s - loss: 1.7919\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 11/100\n",
            "4/4 - 3s - loss: 1.7093\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 12/100\n",
            "4/4 - 3s - loss: 1.6497\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 13/100\n",
            "4/4 - 3s - loss: 1.6669\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 14/100\n",
            "4/4 - 3s - loss: 1.6837\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 15/100\n",
            "4/4 - 3s - loss: 1.6699\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 16/100\n",
            "4/4 - 3s - loss: 1.6358\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 17/100\n",
            "4/4 - 3s - loss: 1.6434\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 18/100\n",
            "4/4 - 3s - loss: 1.6185\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 19/100\n",
            "4/4 - 3s - loss: 1.6010\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 20/100\n",
            "4/4 - 3s - loss: 1.5847\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 21/100\n",
            "4/4 - 3s - loss: 1.5670\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 22/100\n",
            "4/4 - 4s - loss: 1.5731\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 23/100\n",
            "4/4 - 5s - loss: 1.5493\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 24/100\n",
            "4/4 - 5s - loss: 1.5419\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 25/100\n",
            "4/4 - 3s - loss: 1.5208\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 26/100\n",
            "4/4 - 4s - loss: 1.4861\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 27/100\n",
            "4/4 - 3s - loss: 1.4641\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 28/100\n",
            "4/4 - 3s - loss: 1.4513\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 29/100\n",
            "4/4 - 3s - loss: 1.4351\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 30/100\n",
            "4/4 - 3s - loss: 1.4183\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 31/100\n",
            "4/4 - 3s - loss: 1.4034\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 32/100\n",
            "4/4 - 3s - loss: 1.3863\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 33/100\n",
            "4/4 - 3s - loss: 1.3715\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 34/100\n",
            "4/4 - 3s - loss: 1.3548\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 35/100\n",
            "4/4 - 3s - loss: 1.3557\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 36/100\n",
            "4/4 - 3s - loss: 1.3342\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 37/100\n",
            "4/4 - 3s - loss: 1.3227\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 38/100\n",
            "4/4 - 3s - loss: 1.3184\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 39/100\n",
            "4/4 - 3s - loss: 1.3278\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 40/100\n",
            "4/4 - 3s - loss: 1.3755\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 41/100\n",
            "4/4 - 4s - loss: 1.4058\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 42/100\n",
            "4/4 - 3s - loss: 1.3561\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 43/100\n",
            "4/4 - 3s - loss: 1.3343\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 44/100\n",
            "4/4 - 3s - loss: 1.2913\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 45/100\n",
            "4/4 - 3s - loss: 1.2752\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 46/100\n",
            "4/4 - 4s - loss: 1.2546\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 47/100\n",
            "4/4 - 3s - loss: 1.2409\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 48/100\n",
            "4/4 - 3s - loss: 1.2243\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 49/100\n",
            "4/4 - 3s - loss: 1.2066\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 50/100\n",
            "4/4 - 3s - loss: 1.1935\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 51/100\n",
            "4/4 - 3s - loss: 1.1795\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 52/100\n",
            "4/4 - 3s - loss: 1.1702\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 53/100\n",
            "4/4 - 3s - loss: 1.1561\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 54/100\n",
            "4/4 - 3s - loss: 1.1410\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 55/100\n",
            "4/4 - 3s - loss: 1.1297\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 56/100\n",
            "4/4 - 3s - loss: 1.1149\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 57/100\n",
            "4/4 - 3s - loss: 1.1107\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 58/100\n",
            "4/4 - 3s - loss: 1.0999\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 59/100\n",
            "4/4 - 3s - loss: 1.0850\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 60/100\n",
            "4/4 - 3s - loss: 1.0716\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 61/100\n",
            "4/4 - 3s - loss: 1.0591\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 62/100\n",
            "4/4 - 3s - loss: 1.0484\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 63/100\n",
            "4/4 - 3s - loss: 1.0385\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 64/100\n",
            "4/4 - 3s - loss: 1.0284\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 65/100\n",
            "4/4 - 3s - loss: 1.0269\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 66/100\n",
            "4/4 - 3s - loss: 1.0170\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 67/100\n",
            "4/4 - 3s - loss: 1.0036\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 68/100\n",
            "4/4 - 3s - loss: 0.9960\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 69/100\n",
            "4/4 - 3s - loss: 0.9832\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 70/100\n",
            "4/4 - 3s - loss: 0.9792\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 71/100\n",
            "4/4 - 3s - loss: 0.9866\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 72/100\n",
            "4/4 - 3s - loss: 0.9801\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 73/100\n",
            "4/4 - 3s - loss: 0.9782\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 74/100\n",
            "4/4 - 3s - loss: 0.9758\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 75/100\n",
            "4/4 - 3s - loss: 0.9584\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 76/100\n",
            "4/4 - 3s - loss: 0.9605\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 77/100\n",
            "4/4 - 3s - loss: 0.9690\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 78/100\n",
            "4/4 - 3s - loss: 0.9522\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 79/100\n",
            "4/4 - 3s - loss: 0.9300\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 80/100\n",
            "4/4 - 3s - loss: 0.9191\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 81/100\n",
            "4/4 - 3s - loss: 0.9011\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 82/100\n",
            "4/4 - 3s - loss: 0.8858\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 83/100\n",
            "4/4 - 3s - loss: 0.8731\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 84/100\n",
            "4/4 - 3s - loss: 0.8574\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 85/100\n",
            "4/4 - 3s - loss: 0.8457\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 86/100\n",
            "4/4 - 3s - loss: 0.8357\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 87/100\n",
            "4/4 - 3s - loss: 0.8247\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 88/100\n",
            "4/4 - 3s - loss: 0.8246\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 89/100\n",
            "4/4 - 3s - loss: 0.8287\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 90/100\n",
            "4/4 - 3s - loss: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 91/100\n",
            "4/4 - 3s - loss: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 92/100\n",
            "4/4 - 3s - loss: 0.8172\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 93/100\n",
            "4/4 - 3s - loss: 0.7853\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 94/100\n",
            "4/4 - 3s - loss: 0.7805\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 95/100\n",
            "4/4 - 3s - loss: 0.7542\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 96/100\n",
            "4/4 - 3s - loss: 0.7401\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 97/100\n",
            "4/4 - 3s - loss: 0.7259\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 98/100\n",
            "4/4 - 3s - loss: 0.7115\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 99/100\n",
            "4/4 - 3s - loss: 0.7056\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "Epoch 100/100\n",
            "4/4 - 3s - loss: 0.6935\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2b10b30668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}